{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbdIGz7YDQ8q"
   },
   "source": [
    "# ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Gd5PVm7H7o7"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.50.0 datasets==3.5.0 huggingface_hub==0.29.0 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE1cIGmdwT8n"
   },
   "source": [
    "## ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ì—°í•©ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHjb8Rd6DSDh"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "klue_tc_train = load_dataset('klue', 'ynat', split='train')\n",
    "klue_tc_eval = load_dataset('klue', 'ynat', split='validation')\n",
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z90M9sisDUmr"
   },
   "outputs": [],
   "source": [
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpQ3OqhoDWPY"
   },
   "outputs": [],
   "source": [
    "klue_tc_train.features['label'].names\n",
    "# ['ITê³¼í•™', 'ê²½ì œ', 'ì‚¬íšŒ', 'ìƒí™œë¬¸í™”', 'ì„¸ê³„', 'ìŠ¤í¬ì¸ ', 'ì •ì¹˜']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qnb6SqInweAW"
   },
   "source": [
    "## ì‹¤ìŠµì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wr6cX9laDX9Z"
   },
   "outputs": [],
   "source": [
    "klue_tc_train = klue_tc_train.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_eval = klue_tc_eval.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAjvDdjqwoXY"
   },
   "source": [
    "## ì¹´í…Œê³ ë¦¬ë¥¼ ë¬¸ìë¡œ í‘œê¸°í•œ label_str ì»¬ëŸ¼ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2YoqY7jDZVN"
   },
   "outputs": [],
   "source": [
    "klue_tc_train.features['label']\n",
    "# ClassLabel(names=['ITê³¼í•™', 'ê²½ì œ', 'ì‚¬íšŒ', 'ìƒí™œë¬¸í™”', 'ì„¸ê³„', 'ìŠ¤í¬ì¸ ', 'ì •ì¹˜'], id=None)\n",
    "\n",
    "klue_tc_train.features['label'].int2str(1)\n",
    "# 'ê²½ì œ'\n",
    "\n",
    "klue_tc_label = klue_tc_train.features['label']\n",
    "\n",
    "def make_str_label(batch):\n",
    "  batch['label_str'] = klue_tc_label.int2str(batch['label'])\n",
    "  return batch\n",
    "\n",
    "klue_tc_train = klue_tc_train.map(make_str_label, batched=True, batch_size=1000)\n",
    "\n",
    "klue_tc_train[0]\n",
    "# {'title': 'ìœ íŠœë¸Œ ë‚´ë‹¬ 2ì¼ê¹Œì§€ í¬ë¦¬ì—ì´í„° ì§€ì› ê³µê°„ ìš´ì˜', 'label': 3, 'label_str': 'ìƒí™œë¬¸í™”'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzAkPNnmwumM"
   },
   "source": [
    "## í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNbew6U5Da9r"
   },
   "outputs": [],
   "source": [
    "train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)['test']\n",
    "dataset = klue_tc_eval.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "test_dataset = dataset['test']\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd7D7qxEw1mS"
   },
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•œ í•™ìŠµ: (1) ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYfOFc06w37p"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "model_id = \"klue/roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOH98qOLw9yn"
   },
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•œ í•™ìŠµ: (2) í•™ìŠµ ì¸ìì™€ í‰ê°€ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryZVReVmxAn0"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALNfYD95xGsq"
   },
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•œ í•™ìŠµ - (3) í•™ìŠµ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGBbSFcAE2mm"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(test_dataset) # ì •í™•ë„ 0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•œ í•™ìŠµ - (4) ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./results\")\n",
    "tokenizer.save_pretrained(\"./results\")\n",
    "\n",
    "# ì´ë¯¸ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ\n",
    "model_path = \"./results\"\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=\"klue/roberta-base\")\n",
    "\n",
    "# ì‹¤ì œ ë¼ë²¨ ì´ë¦„ ë§¤í•‘\n",
    "label_map = {\n",
    "    \"LABEL_0\": \"ITê³¼í•™\",\n",
    "    \"LABEL_1\": \"ê²½ì œ\",\n",
    "    \"LABEL_2\": \"ì‚¬íšŒ\",\n",
    "    \"LABEL_3\": \"ìƒí™œë¬¸í™”\",\n",
    "    \"LABEL_4\": \"ì„¸ê³„\",\n",
    "    \"LABEL_5\": \"ìŠ¤í¬ì¸ \",\n",
    "    \"LABEL_6\": \"ì •ì¹˜\"\n",
    "}\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
    "test_texts = [\n",
    "    \"ì˜¬í•´ ì™¸êµ­ì¸ ì£¼ì‹ íˆ¬ì ì„±ì , ê°œë¯¸ ìˆ˜ìµë¥ ì˜ 4ë°°ë¡œ ì••ìŠ¹.\",  # ê²½ì œ\n",
    "    \"ì„±ì ì€ 4ìœ„ í•˜ì§€ë§Œ ì¸ê¸°ëŠ” 1ìœ„ ì‚¼ì„±ë¼ì´ì˜¨ì¦ˆ, 2025í”„ë¡œì•¼êµ¬ ê´€ì¤‘ ìˆœìœ„, 82.5%ë¼ëŠ” ë¯¸ì¹œ ì¢Œì„ ì ìœ ìœ¨.\",  # ìŠ¤í¬ì¸ \n",
    "    \"ëŒ€í†µë ¹ 'í•œê¸€ì´ ê·¸ë¦° ì„¸ìƒ, êµ­ë¯¼ì´ ì£¼ì¸ì¸ ë‚˜ë¼ì˜ ë˜ ë‹¤ë¥¸ ëª¨ìŠµ'.\",  # ì •ì¹˜\n",
    "    \"AI ë¶íƒ€ê³  'ì œ 2ì „ì„±ê¸°' ë§ì€ ì´ íšŒì‚¬... í´ë¼ìš°ë“œ ì—…ê³„ ì—”ë¹„ë””ì•„ ë  ìˆ˜ ìˆì„ê¹Œ\",  # ITê³¼í•™\n",
    "    \"ê³ ì†ë„ë¡œ ì‹¤ì‹œê°„ êµí†µìƒí™© 'ë¶€ì‚°->ì„œìš¸ 6ì‹œê°„ 40ë¶„'... ì˜¤í›„ 4~5ì‹œ ì ˆì •.\",  # ìƒí™œë¬¸í™”\n",
    "    \"10ëŒ€ ì‚¬ë§ ìµœëŒ€ ì›ì¸ì€ 'ìì‚´'...10ëŒ€ë„ ì‹¬ë¦¬ë¶€ê²€ ê²€í† í•´ì•¼\"  # ì‚¬íšŒ\n",
    "]\n",
    "\n",
    "# ì˜ˆì¸¡ ì‹¤í–‰\n",
    "for text in test_texts:\n",
    "    result = classifier(text)[0]\n",
    "    label_name = label_map[result[\"label\"]]\n",
    "    print(f\"ğŸ“° ì…ë ¥ ë¬¸ì¥: {text}\")\n",
    "    print(f\"ğŸ“Œ ì˜ˆì¸¡ ê²°ê³¼: {label_name} ({result['score']:.2f})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHVobEq8xS0j"
   },
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•™ìŠµ: (1) í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdTjuT3txUeX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def tokenize_function(examples): # ì œëª©(title) ì»¬ëŸ¼ì— ëŒ€í•œ í† í°í™”\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"klue/roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4I4D0Vs_xamC"
   },
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•™ìŠµ: (2) í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UA738ljxcmh"
   },
   "outputs": [],
   "source": [
    "def make_dataloader(dataset, batch_size, shuffle=True):\n",
    "    dataset = dataset.map(tokenize_function, batched=True).with_format(\"torch\") # ë°ì´í„°ì…‹ì— í† í°í™” ìˆ˜í–‰\n",
    "    dataset = dataset.rename_column(\"label\", \"labels\") # ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\n",
    "    dataset = dataset.remove_columns(column_names=['title']) # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# ë°ì´í„°ë¡œë” ë§Œë“¤ê¸°\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_dataloader = make_dataloader(valid_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = make_dataloader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg5ertQLxtSK"
   },
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•™ìŠµ: (3) í•™ìŠµì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my5ujdBkxvQX"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device) # ëª¨ë¸ì— ì…ë ¥í•  í† í° ì•„ì´ë””\n",
    "        attention_mask = batch['attention_mask'].to(device) # ëª¨ë¸ì— ì…ë ¥í•  ì–´í…ì…˜ ë§ˆìŠ¤í¬\n",
    "        labels = batch['labels'].to(device) # ëª¨ë¸ì— ì…ë ¥í•  ë ˆì´ë¸”\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels) # ëª¨ë¸ ê³„ì‚°\n",
    "        loss = outputs.loss # ì†ì‹¤\n",
    "        loss.backward() # ì—­ì „íŒŒ\n",
    "        optimizer.step() # ëª¨ë¸ ì—…ë°ì´íŠ¸\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqfUWSpYxv6w"
   },
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•™ìŠµ: (4) í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4Vo66qkx0LK"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = np.mean(np.asarray(predictions) == np.asarray(true_labels))\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi1pEAU8x17j"
   },
   "source": [
    "## Trainerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•™ìŠµ: (5) í•™ìŠµ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "m7mXY8iBx6un"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_dataloader)\n",
    "    print(f\"Validation loss: {valid_loss}\")\n",
    "    print(f\"Validation accuracy: {valid_accuracy}\")\n",
    "\n",
    "# Testing\n",
    "_, test_accuracy = evaluate(model, test_dataloader)\n",
    "print(f\"Test accuracy: {test_accuracy}\") # ì •í™•ë„ 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqanaywJyDOq"
   },
   "source": [
    "## í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ëª¨ë¸ ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj62Q5Dm4wDN"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ ì˜ˆì¸¡ ì•„ì´ë””ì™€ ë¬¸ìì—´ ë ˆì´ë¸”ì„ ì—°ê²°í•  ë°ì´í„°ë¥¼ ëª¨ë¸ configì— ì €ì¥\n",
    "id2label = {i: label for i, label in enumerate(train_dataset.features['label'].names)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03EY1lJ0ISzC"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"ë³¸ì¸í—ˆê¹…í˜ì´ìŠ¤í† í°\")\n",
    "repo_id = f\"í—ˆê¹…í˜ì´ìŠ¤ID/roberta-base-klue-ynat-classification\"\n",
    "# Trainerë¥¼ ì‚¬ìš©í•œ ê²½ìš°\n",
    "trainer.push_to_hub(repo_id)\n",
    "# ì§ì ‘ í•™ìŠµí•œ ê²½ìš°\n",
    "model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/onlybooks/llm/blob/main/03%E1%84%8C%E1%85%A1%E1%86%BC/chapter_3.ipynb",
     "timestamp": 1746594896221
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
